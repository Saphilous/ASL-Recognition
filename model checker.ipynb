{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\">American Sign Language Recognition Using Tensorflow, Keras, and OpenCV</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before diving into the code, we first have to import all the required libraries. I'll use Tensorflow, Keras, and OpenCv. We're also importing numpy which will help us reshae the image frame from the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.5.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow as tf\n",
    "print(cv2.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that's done, we have to import our Machine Learning model into our file. Keras provides us with a module that makes this quite simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('model1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accumulated weight will be useful if we're predicting foreground masks instead of images themselves. However, since we trained our model on images, this won't be necessary.\n",
    "\n",
    "Region of Interest is the region we want our model to see. I only made a small portion of the frame as region of interest so my body won't be visible when I'm projecting the signs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "background = None\n",
    "accumulated_weight = 0.5\n",
    "\n",
    "ROI_top = 100\n",
    "ROI_bottom = 300\n",
    "ROI_left = 150\n",
    "ROI_right = 350"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to the interesting part. We're starting video capture using OpenCV and obtaining our frame. We're then converting it into a grayscale image and resizing it. Numpy comes in handy here during reshaping.\n",
    "\n",
    "We're then calling the predict function from our model to preidct the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'a', 1: 'b', 2: 'c', 3: 'd', 4: 'e', 5: 'f', 6: 'g', 7: 'h', 8: 'i', 9: 'j', 10: 'k', 11: 'l', 12: 'm', 13: 'n', 14: 'o', 15: 'p', 16: 'q', 17: 'r', 18: 's', 19: 't', 20: 'u', 21: 'v', 22: 'w', 23: 'x', 24: 'y', 25: 'z', 26: 'space', 27: 'nothing'}\n"
     ]
    }
   ],
   "source": [
    "from cv2 import VideoCapture\n",
    "\n",
    "word_dict = {}\n",
    "\n",
    "for n in range(0, 26):\n",
    "    word_dict[n] = chr(97 + n)\n",
    "\n",
    "word_dict[26] = 'space'\n",
    "word_dict[27] = 'nothing'\n",
    "\n",
    "print(word_dict)\n",
    "\n",
    "cam = VideoCapture(0)\n",
    "num_frames = 0\n",
    "\n",
    "\n",
    "while True:\n",
    "    ret, frame = cam.read()\n",
    "\n",
    "    if not ret:\n",
    "        print(\"failed to grab frame\")\n",
    "        break\n",
    "\n",
    "    # filpping the frame to prevent inverted image of captured frame...\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    frame_copy = frame.copy()\n",
    "\n",
    "    # ROI from the frame\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    gray = gray[ROI_top:ROI_bottom, ROI_left:ROI_right]\n",
    "    cv2.imshow(\"ROI\", gray)\n",
    "\n",
    "    thresholded = cv2.resize(gray, (64, 64))\n",
    "    thresholded = cv2.cvtColor(thresholded, cv2.COLOR_GRAY2RGB)\n",
    "    thresholded = cv2.GaussianBlur(thresholded, (5, 5), 0)\n",
    "    thresholded = thresholded.reshape(1, 64, 64, 3)\n",
    "\n",
    "    pred = model.predict(thresholded)\n",
    "    cv2.putText(frame_copy, str(word_dict[(np.argmax(pred))]) , (100, 100), cv2.FONT_HERSHEY_SIMPLEX, 3, (0, 0, 0), 2)\n",
    "    cv2.imshow(\"Sign_Detection\", frame_copy)\n",
    "    if(cv2.waitKey(1) == ord('q')):\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ta da! Its done! There, we have our first ever (well, mine atleast :) ) sign language recongition model\n",
    "\n",
    "Don't forget to destroy the opencv window once its done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cf350fe05fa96dc25849aab245a76667d10039810e6fcc4a2ea7c45e4c52cab4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
